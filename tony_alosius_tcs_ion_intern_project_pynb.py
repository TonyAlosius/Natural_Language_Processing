# -*- coding: utf-8 -*-
"""Tony_Alosius_TCS_iON_Intern_Project.pynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CbZve8_MfEkV6ZiKafwJ9afcRbSTHlGd

# AUOTOMATE DETECTION OF DIFFERENT EMOTIONS FROM TEXTUAL COMMENTS AND FEEDBACKS
"""

# Required Packages for Project 
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from numpy import array
from keras.preprocessing.text import one_hot
# pad_sequences is used to ensure that all sequences in a list have the same length. 
# By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers import GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer

from google.colab import drive
drive.mount('/content/drive')
data=pd.read_csv('drive/My Drive/TCS-ION/tweeter_emotions.csv')

# Drop the Tweet_Id which is not neccesary for our Analysis
data.drop(columns="tweet_id", inplace=True)

# Pre-Processing the entire data at a Single step using the def function:
def preprocess_text(sen):
    sentence = remove_tags(sen)
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)
    sentence = re.sub(r'\s+', ' ', sentence)
    return sentence

# Remove the tags from the document
Remove_Tags = re.compile(r'<[^>]+>')
def remove_tags(text):
    return Remove_Tags.sub('', text)

# Total number of positive and hegative words in the dataset
data["sentiment"].value_counts()

data["content"] = data["content"].apply(lambda x: x.lower())

# Import the Natural Language Tool Kit
import nltk
from nltk.corpus import stopwords

# Handling the stop words using th lambda function
nltk.download('stopwords')
sw_list = stopwords.words("english")
data['content'] = data['content'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:" ".join(x))

# Slicing into x and y 
X = data.iloc[:,1]
y = data["sentiment"]

# Encoding the Dependent Variable
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

# Splitting into traing and testing set using in-bulit pyhton functions
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)

# tokenization is the process of converting a sequence of characters into a sequence of lexical tokens
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Padding the vocabulary size 
vocab_size = len(tokenizer.word_index) + 1
maxlen = 100
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

from numpy import array
from numpy import asarray
from numpy import zeros

# GloVe - Global Vector for Word Representation
embeddings_dictionary = dict()
glove_file = open('drive/My Drive/TCS-ION/glove.txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
glove_file.close()

# Embedding into a matrix
embedding_matrix = zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    # Embedding the matric into vector
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

# Creating a Sequence -Sequential() of layers of neurons using Deep learning model
model = Sequential()
embedding_layers = Embedding(vocab_size, 100, weights=[embedding_matrix], 
                             input_length=maxlen , 
                             trainable=False)
model.add(embedding_layers)
# Converting into a unidirectional array - Flatten()
model.add(Flatten())
model.add(Dense(1, activation='softmax'))

# loss='binary_crossentropy'
# optimizer='adam'
# metrics=['accuracy']

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(X_train, y_train, batch_size=128, epochs=50, verbose=1, validation_split=0.2)

# FInding the Accuracy Of our Model
Evaluation_score = model.evaluate(X_test, y_test, verbose=1)
print("Test Score:", Evaluation_score[0])
print("Test Accuracy:", Evaluation_score[1])

# Validating the model 
instance_sam = X[50]
print(instance_sam)
print("Tony_Alosius_TCS_iON_Project")

instance_sam = tokenizer.texts_to_sequences(instance_sam)

flat_list = []
for sublist in instance_sam:
    for item in sublist:
        flat_list.append(item)

flat_list = [flat_list]

instance_sam = pad_sequences(flat_list, padding='post', maxlen=maxlen)

pred=model.predict(instance_sam)
if pred[0]>=0.6:
  print("positive")
elif pred[0]>0.5 and pred[0]<0.6:
  print("neutral")
else:
  print("negative")